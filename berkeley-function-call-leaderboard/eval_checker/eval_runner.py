import sys

sys.path.append("../")

from checker import ast_checker, exec_checker, executable_checker_rest
from eval_runner_helper import *
from tqdm import tqdm
import argparse


# NOTE: This file should be run in the `eval_checker` directory
# 1. single_executable_file_runner
# 2. single_relevance_file_runner
# 3. single_ast_file_runner : normal ast test
import pandas as pd
import json


def check_name_for_buildin_function(possible_answer_item, model_result_item):
    """for buildin function, there are no arguments, only check name
    æ­¤å¤„possible_answer_itemå’Œmodel_result_iteméƒ½æ˜¯dictï¼Œä¸”éƒ½ä¸ç©ºï¼ˆä¹‹å‰å·²æ£€æŸ¥
    """
    if "arguments" in model_result_item:
        # æ¨¡å‹ä¸åº”å¯¹å†…ç½®å‡½æ•° äº§ç”Ÿ arguments
        return False
    if possible_answer_item["name"] == model_result_item["name"]:
        return True
    return False


def check_input_file_empty(file_path):
    try:
        with open(file_path, "r") as f:
            return bool(
                f.readline().strip()
            )  # Read the first line and check if it's non-empty
    except FileNotFoundError:
        return False  # File doesn't exist


def clean_and_convert_excel_to_jsonl(file_path, output_path):
    """
    å°†Excelæ–‡ä»¶æ¸…æ´—å¹¶è½¬æ¢ä¸ºJSONLæ ¼å¼æ–‡ä»¶ã€‚

    Args:
        file_path (str): Excelæ–‡ä»¶çš„è·¯å¾„ã€‚
        output_path (str): è¾“å‡ºçš„JSONLæ–‡ä»¶è·¯å¾„ã€‚

    Returns:
        None: æ­¤å‡½æ•°æ— è¿”å›å€¼ï¼Œä¼šå°†æ¸…æ´—åçš„æ•°æ®å†™å…¥åˆ°æŒ‡å®šçš„JSONLæ–‡ä»¶ä¸­ã€‚

    """
    # Load the Excel file
    excel_data = pd.read_excel(file_path)

    # Function to clean and convert the string representation of lists/dicts
    def clean_data(record):
        cleaned_record = {}
        for key, value in record.items():
            print(f" current key: {key}")
            print(f" current value: {value}")
            cleaned_value = json.loads(value)
            cleaned_record[key] = cleaned_value
        return cleaned_record

    # Clean the data
    jsonl_data = excel_data.to_dict(orient="records")
    cleaned_jsonl_data = [clean_data(record) for record in jsonl_data]

    # Write the cleaned JSONL file
    with open(output_path, "w") as jsonl_file:
        for record in cleaned_jsonl_data:
            jsonl_file.write(json.dumps(record, ensure_ascii=False) + "\n")


def convert_to_BFCL_format_input(
    original_yewu_file: str,
    converted_simple_func_description: str,
    converted_multiple_func_description: str,
    converted_simple_model_result: str,
    converted_multiple_model_result: str,
    converted_simple_possible_answer: str,
    converted_multiple_possible_answer: str,
):
    """å°†åŸå§‹ä¸šåŠ¡jsonl æ‹†åˆ†ä¸ºsimpleå’Œmultipleä¸¤ç±»ï¼Œ
    æ¯ç±»éœ€function_descriptionã€model_resultã€possible_answer3ä¸ªæ–‡ä»¶
    """
    with open(original_yewu_file, "r") as originial_file, open(
        converted_simple_func_description, "w"
    ) as simple_file, open(
        converted_multiple_func_description, "w"
    ) as multiple_file, open(
        converted_simple_model_result, "w"
    ) as simple_model_result_file, open(
        converted_multiple_model_result, "w"
    ) as multiple_model_result_file, open(
        converted_simple_possible_answer, "w"
    ) as simple_possible_answer_file, open(
        converted_multiple_possible_answer, "w"
    ) as multiple_possible_answer_file:

        data = originial_file.readlines()

        for line in data:
            line = json.loads(line)

            if len(line["tools"]) > 1:  # multiple

                func_descrp = {"question": "", "function": []}
                for tool in line["tools"]:
                    if "function" not in tool:  # å†…éƒ¨å‡½æ•°ï¼Œå¦‚web_search
                        func_descrp["function"].append(
                            tool
                        )  # ç›´æ¥append {"type": "web_search"}, if "type" in  é‚£è¯´æ˜å°±æ˜¯å†…éƒ¨å‡½æ•°

                    else:
                        func_descrp["function"].append(
                            tool["function"]
                        )  # appendçš„æ˜¯ {"name": "", "description": "", "parameters": }çš„dict
                multiple_file.write(json.dumps(func_descrp, ensure_ascii=False) + "\n")

                if line["steps"]:  # è¯´æ˜æ¨¡å‹è®¤ä¸ºä½¿ç”¨tools
                    model_result = {"result": line["steps"][0]}
                else:  # è¯´æ˜æ¨¡å‹è®¤ä¸ºä¸ä½¿ç”¨tools
                    model_result = {
                        "result": {}
                    }  # æ¨¡å‹è®¤ä¸ºä¸ä½¿ç”¨toolsï¼Œé‚£ä¹ˆå°±æ˜¯ç©ºçš„dict

                multiple_model_result_file.write(
                    json.dumps(model_result, ensure_ascii=False) + "\n"
                )

                if line["expected_steps"]:  # è¯´æ˜ç­”æ¡ˆä¹Ÿä½¿ç”¨äº†å·¥å…·
                    possible_answer = line["expected_steps"][0]
                else:
                    possible_answer = {}  # æ²¡æœ‰ä½¿ç”¨å·¥å…·ï¼Œé‚£ä¹ˆå°±æ˜¯ç©ºçš„dict

                multiple_possible_answer_file.write(
                    json.dumps(possible_answer, ensure_ascii=False) + "\n"
                )

            else:  # simple, tool len ==1

                func_descrp = {"question": "", "function": []}

                if "function" not in line["tools"][0]:  # å†…éƒ¨å‡½æ•°ï¼Œå¦‚web_search
                    func_descrp["function"].append(line["tools"][0])
                else:
                    func_descrp["function"].append(line["tools"][0]["function"])

                simple_file.write(json.dumps(func_descrp, ensure_ascii=False) + "\n")
                if line["steps"]:  # è¯´æ˜æ¨¡å‹è®¤ä¸ºä½¿ç”¨tools
                    model_result = {"result": line["steps"][0]}
                else:  # è¯´æ˜æ¨¡å‹è®¤ä¸ºä¸ä½¿ç”¨tools
                    model_result = {
                        "result": {}
                    }  # æ¨¡å‹è®¤ä¸ºä¸ä½¿ç”¨toolsï¼Œé‚£ä¹ˆå°±æ˜¯ç©ºçš„dict
                simple_model_result_file.write(
                    json.dumps(model_result, ensure_ascii=False) + "\n"
                )

                if line["expected_steps"]:  # è¯´æ˜ç­”æ¡ˆä¹Ÿä½¿ç”¨äº†å·¥å…·
                    possible_answer = line["expected_steps"][0]
                else:
                    possible_answer = {}  # æ²¡æœ‰ä½¿ç”¨å·¥å…·ï¼Œé‚£ä¹ˆå°±æ˜¯ç©ºçš„dict
                simple_possible_answer_file.write(
                    json.dumps(possible_answer, ensure_ascii=False) + "\n"
                )


def single_executable_file_runner(
    handler, model_result, func_description, model_name, test_category
):
    assert len(model_result) == len(func_description)

    result = []
    correct_count = 0
    for i in tqdm(range(len(model_result)), desc="Running tests"):
        try:
            raw_result = model_result[i]["result"]
        except Exception as e:

            print(
                f"Error: {e} for model {model_name} and test category {test_category} at index {i}, model_result[i] is {model_result[i]}"
            )

        try:
            decoded_result = handler.decode_execute(raw_result)
        except Exception as e:
            print(
                f"decoed error: {e} for model {model_name} and test category {test_category} at index {i}, raw_result is {raw_result} "
            )
            result.append(
                {
                    "id": i + 1,
                    "model_name": model_name,
                    "test_category": test_category,
                    "valid": False,
                    "error": [f"Failed to decode executable. {str(e)}"],
                    "error_type": "executable_decoder:decoder_failed",
                    "func_description": func_description[i],
                    "model_result_raw": raw_result,
                }
            )
            continue

        if "rest" in test_category:
            # REST is always single-functioned. Therefore we take the first one and pass it to the REST checker.
            if not is_rest_format_output(decoded_result):
                result.append(
                    {
                        "id": i + 1,
                        "model_name": model_name,
                        "test_category": test_category,
                        "valid": False,
                        "error": [
                            "Did not output in the specified format. Note: the model_result is wrapped in a string to ensure json serializability."
                        ],
                        "error_type": "executable_decoder:rest_wrong_output_format",
                        "func_description": func_description[i],
                        "model_result_raw": str(raw_result),
                        "model_result_decoded": str(decoded_result),
                    }
                )
                continue

            checker_result = executable_checker_rest(decoded_result[0], i)

        else:
            if not is_executable_format_output(decoded_result):
                result.append(
                    {
                        "id": i + 1,
                        "model_name": model_name,
                        "test_category": test_category,
                        "valid": False,
                        "error": [
                            "Did not output in the specified format. Note: the model_result is wrapped in a string to ensure json serializability."
                        ],
                        "error_type": "executable_decoder:wrong_output_format",
                        "func_description": func_description[i],
                        "model_result_raw": str(raw_result),
                        "model_result_decoded": str(decoded_result),
                    }
                )
                continue

            func_description_item = func_description[i]
            # print(f"before exec_checker: {decoded_result}")
            checker_result = exec_checker(
                decoded_result, func_description_item, test_category
            )
            # print(f"after exec_checker: {checker_result}")

        if checker_result["valid"]:
            correct_count += 1
        else:
            temp = {}
            temp["id"] = i + 1
            temp["model_name"] = model_name
            temp["test_category"] = test_category
            temp["valid"] = checker_result["valid"]
            temp["error"] = checker_result["error"]
            temp["error_type"] = checker_result["error_type"]
            temp["func_description"] = func_description[i]
            temp["model_result_raw"] = raw_result
            temp["model_result_decoded"] = decoded_result
            if "model_executed_output" in checker_result:
                temp["model_executed_output"] = checker_result["model_executed_output"]
            result.append(temp)

    accuracy = correct_count / len(model_result)
    result.insert(
        0,
        {
            "accuracy": accuracy,
            "correct_count": correct_count,
            "total_count": len(model_result),
        },
    )
    output_file_name = test_category + "_score.json"
    output_file_dir = os.path.join(OUTPUT_PATH, model_name)
    write_list_of_dicts_to_file(output_file_name, result, output_file_dir)

    return accuracy, len(model_result)


def single_relevance_file_runner(handler, model_result, model_name, test_category):

    result = []
    correct_count = 0
    for i in range(len(model_result)):
        model_result_item = model_result[i]["result"]
        success = False
        decoded_result = None

        try:
            decoded_result = handler.decode_ast(
                model_result_item, language="Python"
            )  # å°±æ˜¯ç©ºçš„æ‰å¯¹
            success = False
            if is_empty_output(decoded_result):
                success = True

        except Exception as e:
            success = True

        if success:
            correct_count += 1
        else:
            temp = {}
            temp["id"] = i + 1
            temp["model_name"] = model_name
            temp["test_category"] = test_category
            temp["valid"] = success
            temp["error"] = [
                f"Valid syntax. Successfully decode AST when it should not."
            ]
            temp["error_type"] = "relevance_error:decoder_success"
            temp["model_result"] = model_result_item
            temp["decoded_result"] = decoded_result

            result.append(temp)

    accuracy = correct_count / len(model_result)
    result.insert(
        0,
        {
            "accuracy": accuracy,
            "correct_count": correct_count,
            "total_count": len(model_result),
        },
    )
    output_file_name = test_category + "_score.json"
    output_file_dir = os.path.join(OUTPUT_PATH, model_name)
    write_list_of_dicts_to_file(output_file_name, result, output_file_dir)

    return accuracy, len(model_result)


def single_ast_file_runner(
    handler,
    model_result,
    func_description,
    possible_answer,
    language,
    test_category,
    model_name,
):
    """
    model_result: a list of either :

        {"result": {"name": "web_search"}}
        {"result": {"name": "ExpertSearch", "arguments": {"query": "å•åŒæ›²é“å•æ¿ä»·æ ¼"}}}

        func_description = load_file(func_description_file)   a list of :
        {"question": "", "function": [{"type": "web_search"}, {"name": "ExpertSearch", "description": "ç§åŸŸæœç´¢å·¥å…·ï¼Œå½“éœ€è¦æœç´¢å•†å®¶æŸ±å½¢é“å•æ¿ã€å†²å­”é“å•æ¿ã€çŸ³çº¹é“å•æ¿ã€æœ¨çº¹é“å•æ¿ã€å•åŒæ›²é“å•æ¿ã€é›•èŠ±é“å•æ¿ã€å¹•å¢™é“å•æ¿ã€ç°åœºæµ‹é‡ã€å‡ºå›¾è®¾è®¡ã€æŠ€æœ¯æ”¯æŒã€å®šåˆ¶æœåŠ¡ç›¸å…³ä¿¡æ¯æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·ã€‚", "parameters": {"type": "object", "properties": {"query": {"type": "string", "description": "æœç´¢é—®é¢˜"}}, "required": ["query"]}}]}
        {"question": "", "function": [{"type": "web_search"}]}

        possible_answer = load_file(possible_answer_file)  a list of :[....]
        {"name": "web_search"}
        {"name": "ExpertSearch", "arguments": {"query": ["å•åŒæ›²é“å•æ¿ä»·æ ¼"]}}
    """

    assert (
        len(model_result) == len(func_description) == len(possible_answer)
    ), "The length of the model result does not match the length of the func_description or possible answer. Please check the input files for completeness."

    result = []
    correct_count = 0
    # start
    for i in range(len(model_result)):

        model_result_item = model_result[i]["result"]
        # {"name": "web_search"} -- å†…éƒ¨å‡½æ•°
        # {"name": "ExpertSearch", "arguments": {"query": "å•åŒæ›²é“å•æ¿ä»·æ ¼"}}
        func_description_item = func_description[i]["function"]
        #  [{"type": "web_search"}, {"name": "ExpertSearch", "description": "ç§.........
        # [{"type": "web_search"}]
        possible_answer_item = possible_answer[i]
        # {"name": "web_search"}
        # {"name": "ExpertSearch", "arguments": {"query": ["å•åŒæ›²é“å•æ¿ä»·æ ¼"]}}

        # ä»ç­”æ¡ˆå…¥æ‰‹ï¼Œ å…ˆæ£€æŸ¥ä¸€äº›cornor case
        # æ¨¡å‹ç©ºï¼Œç­”æ¡ˆç©ºï¼Œæ­£ç¡®
        # æ¨¡å‹ç©ºï¼Œç­”æ¡ˆä¸ç©ºï¼Œé”™è¯¯
        # æ¨¡å‹ä¸ç©ºï¼Œç­”æ¡ˆç©ºï¼Œé”™è¯¯
        # æ¨¡å‹ä¸ç©ºï¼Œç­”æ¡ˆä¸ç©ºï¼Œï¼Œæ£€æŸ¥name
        if (
            not possible_answer_item and model_result_item
        ):  # æ¨¡å‹ä¸ç©ºï¼Œç­”æ¡ˆç©ºï¼Œ é‚£è‚¯å®šé”™çš„
            result.append(
                {
                    "id": i + 1,
                    "model_name": model_name,
                    "test_category": test_category,
                    "valid": False,
                    "error": [
                        "The possible answer is empty,but the model output is not."
                    ],
                    "error_type": "ast_checker:empty_possible_answer",
                    "func_description": func_description[i],
                    "model_result_raw": model_result_item,
                    "possible_answer": possible_answer_item,
                }
            )
            continue
        elif not model_result_item and possible_answer_item:  # æ¨¡å‹ç©ºï¼Œç­”æ¡ˆä¸ç©ºï¼Œé”™è¯¯
            result.append(
                {
                    "id": i + 1,
                    "model_name": model_name,
                    "test_category": test_category,
                    "valid": False,
                    "error": [
                        "The model output is empty,but the possible answer is not."
                    ],
                    "error_type": "ast_checker:empty_model_output",
                    "func_description": func_description[i],
                    "model_result_raw": model_result_item,
                    "possible_answer": possible_answer_item,
                }
            )
            continue
        elif not possible_answer_item and not model_result_item:  # ç­”æ¡ˆå’Œæ¨¡å‹éƒ½ç©ºï¼Œæ­£ç¡®
            correct_count += 1
            continue

        # å‰©ä¸‹ æ¨¡å‹å’Œç­”æ¡ˆéƒ½ä¸ç©ºçš„æƒ…å†µï¼š

        # å…ˆåˆ¤æ–­ poissible answer æ˜¯å¦æ˜¯å†…ç½®å‡½æ•°ï¼Œå¦‚æœæ˜¯å†…ç½®å‡½æ•°ï¼Œé‚£ä¹ˆå°±ä¸éœ€è¦æ£€æŸ¥argumentsï¼Œåªéœ€è¦æ£€æŸ¥name
        if "arguments" not in possible_answer_item and "name" in possible_answer_item:
            # è¯´æ˜æ˜¯ å†…ç½®å‡½æ•°å¦‚websearch å°±æ˜¯å†…ç½®å‡½æ•°  ï¼Œæ²¡æœ‰arguments, åªéœ€è¦æ£€æŸ¥name
            checker_result = check_name_for_buildin_function(
                possible_answer_item, model_result_item
            )
            if checker_result:
                correct_count += 1
                continue
            else:
                result.append(
                    {
                        "id": i + 1,
                        "model_name": model_name,
                        "test_category": test_category,
                        "valid": False,
                        "error": [
                            "The possible answer is a buildin function, but the model output does not satisfy the requirement."
                        ],
                        "error_type": "ast_checker:buildin_function",
                        "func_description": func_description[i],
                        "model_result_raw": model_result_item,
                        "possible_answer": possible_answer_item,
                    }
                )
                continue

        # å‰©ä¸‹ç­”æ¡ˆæ˜¯è‡ªå®šä¹‰å‡½æ•°çš„æƒ…å†µï¼Œå¯ä»¥decode astï¼Œç„¶åæ£€æŸ¥name å’Œarguments
        try:
            model_result_item_raw = model_result_item
            print(f"before decode: model_result_item : {model_result_item}")
            model_result_item = handler.decode_ast(model_result_item, language)
            # ä» {'name': 'database_query', 'arguments': {'table': 'user', 'conditions': [{'field': 'aaaa', 'operation': '>', 'value': '25'}, {'field': 'job', 'operation': '=', 'value': 'engineer'}]}} å˜æˆ:
            # [{'database_query': {'table': 'user', 'conditions': [{'field': 'aaaa', 'operation': '>', 'value': '25'}, {'field': 'job', 'operation': '=', 'value': 'engineer'}]}}] ---list of 1 dictï¼Œ or [{'calculate_triangle_area': {'base': 10, 'height': 5}}]
            # ä¹Ÿæ˜¯ä¸€ä¸ªä¸œè¥¿ï¼Œæ¨¡å‹åªä¼šå¤šé€‰ä¸€

        except Exception as e:  # ASTè§£ç å¤±è´¥
            if str(e) == "'arguments'":
                error = [
                    "Invalid syntax. Failed to decode AST. Model use build in function which miss 'arguments' key that cannot be decoded."
                ]
            else:
                error = [f"Invalid syntax. Failed to decode AST. {str(e)}"]
            result.append(
                {
                    "id": i + 1,
                    "model_name": model_name,
                    "test_category": test_category,
                    "valid": False,
                    "error": error,
                    "error_type": "ast_decoder:decoder_failed",
                    "func_description": func_description[i],
                    "model_result_raw": model_result_item_raw,
                    "possible_answer": possible_answer_item,
                }
            )
            continue

        decoder_output_valid = is_function_calling_format_output(
            model_result_item
        )  # Ensure the output is a list of dictionaries --[{"ExpertSearch": {"query": "å•åŒæ›²é“å•æ¿ä»·æ ¼"}}]
        if not decoder_output_valid:
            # ASTè§£ç æˆåŠŸï¼Œä½†æ˜¯è§£ç åçš„æ ¼å¼ä¸å¯¹, ä¸æ˜¯list of dict
            result.append(
                {
                    "id": i + 1,
                    "model_name": model_name,
                    "test_category": test_category,
                    "valid": False,
                    "error": [
                        "Did not output in the specified format. Note: the model_result is wrapped in a string to ensure json serializability."
                    ],
                    "error_type": "ast_decoder:decoder_wrong_output_format",
                    "func_description": func_description[i],
                    "model_result_raw": str(model_result_item_raw),
                    "model_result_decoded": str(model_result_item),
                    "possible_answer": possible_answer_item,
                }
            )
            continue

        # ASTè§£ç æˆåŠŸï¼Œä¸”æ ¼å¼æ­£ç¡®ï¼Œæ£€æŸ¥name å’Œarguments çš„type ï¼Œä¸åšvalueæ£€æŸ¥

        possible_answer_item = {
            possible_answer_item["name"]: possible_answer_item["arguments"]
        }
        #  åˆ°è¿™é‡Œæ‰è½¬æ¢ä¸ºBFCL åŸå§‹çš„possible answeræ ¼å¼ï¼Œå› ä¸ºå‰é¢è¦æ£€æŸ¥å†…ç½®å‡½æ•°éœ€è¦arguments è¿™ä¸ªkey å­˜åœ¨
        #  å˜æˆ{"ExpertSearch": {"query": ["å•åŒæ›²é“å•æ¿ä»·æ ¼"]}}, æ³¨æ„æ¯ä¸ªparamçš„possilble valueæ˜¯ä¸ªlist
        checker_result = ast_checker(
            func_description_item,  # å³list of tool poolï¼Œ
            # ä½†è¿™é‡Œtool pool ä¸­è¿˜å¯èƒ½å¸¦ç€å†…ç½®å‡½æ•°ï¼Œå› ä¸ºæ˜¯pool ä¸­çš„å‡½æ•°ï¼Œä½†æ— æ‰€è°“ï¼š [{"type": "web_search"}, {"name": "ExpertSearch", "description": "ç§.........
            model_result_item,  # å·²ç»å˜æˆäº† list of dictï¼Œè¿™é‡Œä¸å¯èƒ½æœ‰å†…ç½®å‡½æ•°ï¼Œæ‰€ä»¥ä¸ä¼šæŠ¥é”™ï¼š [{'database_query': {'table': 'user', 'conditions': [{'field': 'aaaa', 'operation': '>', 'value': '25'}, {'field': 'job', 'operation': '=', 'value': 'engineer'}]}}]
            possible_answer_item,  # ä¸å¯èƒ½æ˜¯å†…ç½®å‡½æ•°ï¼Œ{'database_query': {'table': ['user'], 'conditions': [[{'field': ['age'], 'operation': ['>'], 'value': ['25']}, {'field': ['job'], 'operation': ['='], 'value': ['engineer']}]]}}
            language,
            test_category,
            model_name,
        )

        if checker_result["valid"]:
            correct_count += 1
        else:
            temp = {}
            temp["id"] = i + 1
            temp["model_name"] = model_name
            temp["test_category"] = test_category
            temp["valid"] = checker_result["valid"]
            temp["error"] = checker_result["error"]
            temp["error_type"] = checker_result["error_type"]
            temp["func_description"] = func_description[i]
            temp["model_result_raw"] = model_result_item_raw
            temp["model_result_decoded"] = model_result_item
            # temp["possible_answer"] = possible_answer_item
            result.append(temp)

    accuracy = correct_count / len(model_result)
    result.insert(
        0,
        {
            "accuracy": accuracy,
            "correct_count": correct_count,
            "total_count": len(model_result),
        },
    )
    output_file_name = test_category + "_score.json"
    output_file_dir = os.path.join(OUTPUT_PATH, model_name)
    write_list_of_dicts_to_file(output_file_name, result, output_file_dir)

    return accuracy, len(model_result)


#### Main runner function ####
def runner(
    model_names: str,
    test_categories: list,
    simple_func_file: str,
    multiple_func_file: str,
    simple_model_result_file: str,
    multiple_model_result_file: str,
    simple_possible_answer: str,
    multiple_possible_answer: str,
):

    # 0.
    for test_category in test_categories:
        # ['simple', 'multiple_function'] or ['simple'] or ['multiple_function']
        if test_category == "simple":
            model_result_json = simple_model_result_file  # {"name": "calculate_derivative", "arguments": "{\"function\":\"3x^2 + 2x - 1\"}"}
            func_description_file = simple_func_file
            possible_answer_file = simple_possible_answer

        elif test_category == "multiple_function":
            model_result_json = multiple_model_result_file
            func_description_file = multiple_func_file
            possible_answer_file = multiple_possible_answer
        else:
            print(f"Unsupported test category: {test_category}")
            continue

        # 1. first check if any of the input file is emptyï¼Œ
        # since user's excel might only support simple or multiple, but not both. Thus one of the input file might be empty
        valid_flag = check_input_file_empty(model_result_json)
        if not valid_flag:  # å¦‚æœæŸä¸ªcategoryçš„fileæ˜¯ç©ºçš„ï¼Œé‚£ä¹ˆå°±ä¸ç”¨è·‘äº†
            print(
                f"âš ï¸ The input file for test category {test_category} is empty. Skipping the evaluation. \n Your original excel file does not fit in this test category."
            )
            continue

        handler = get_handler(model_names)
        language = "Python"
        print(f"ğŸ” Running test: {test_category}")

        # 2. run the test
        model_result = load_file(model_result_json)  # returns a list of model output
        # a list of either :
        # {"result": {"name": "web_search"}}
        # {"result": {"name": "ExpertSearch", "arguments": {"query": "å•åŒæ›²é“å•æ¿ä»·æ ¼"}}}
        func_description = load_file(func_description_file)
        # a list of :
        # {"question": "", "function": [{"type": "web_search"}, {"name": "ExpertSearch", "description": "ç§åŸŸæœç´¢å·¥å…·ï¼Œå½“éœ€è¦æœç´¢å•†å®¶æŸ±å½¢é“å•æ¿ã€å†²å­”é“å•æ¿ã€çŸ³çº¹é“å•æ¿ã€æœ¨çº¹é“å•æ¿ã€å•åŒæ›²é“å•æ¿ã€é›•èŠ±é“å•æ¿ã€å¹•å¢™é“å•æ¿ã€ç°åœºæµ‹é‡ã€å‡ºå›¾è®¾è®¡ã€æŠ€æœ¯æ”¯æŒã€å®šåˆ¶æœåŠ¡ç›¸å…³ä¿¡æ¯æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·ã€‚", "parameters": {"type": "object", "properties": {"query": {"type": "string", "description": "æœç´¢é—®é¢˜"}}, "required": ["query"]}}]}
        #    {"question": "", "function": [{"type": "web_search"}]}

        possible_answer = load_file(possible_answer_file)
        # a list of :
        # {"name": "web_search"}
        # {"name": "ExpertSearch", "arguments": {"query": ["å•åŒæ›²é“å•æ¿ä»·æ ¼"]}}

        record_cost_latency(LEADERBOARD_TABLE, model_names, model_result)

        accuracy, total_count = single_ast_file_runner(
            handler,
            model_result,
            func_description,
            possible_answer,
            language,
            test_category,
            model_names,
        )
        record_result(
            LEADERBOARD_TABLE, model_names, test_category, accuracy, total_count
        )
        print(f"âœ… Test completed: {test_category}. ğŸ¯ Accuracy: {accuracy}")

    # This function reads all the score files from local folder and updates the leaderboard table.
    # This is helpful when you only want to run the evaluation for a subset of models and test categories.
    update_leaderboard_table_with_score_file(LEADERBOARD_TABLE, OUTPUT_PATH)
    # Write the leaderboard table to a file
    generate_leaderboard_csv(LEADERBOARD_TABLE, OUTPUT_PATH)

    # Clean up the executable expected output files
    # They should be re-generated the next time the evaluation is run
    # clean_up_executable_expected_output(
    #     func_description_PATH, EXECUTABLE_TEST_CATEGORIES_HAVE_RUN
    # )


ARG_PARSE_MAPPING = {
    "all": [
        "simple",
        "multiple_function",
    ],
}


# INPUT_PATH = "../result/"
# func_description_PATH = "../data/"
# POSSIBLE_ANSWER_PATH = "../data/possible_answer/"
OUTPUT_PATH = "../score/"

# A dictionary to store the results
# Key is model name, value is a dictionary with keys as test category and values as a dictionary with accuracy and total count
LEADERBOARD_TABLE = {}


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process two lists of strings.")

    # ä¼ å…¥excel æ–‡ä»¶
    parser.add_argument(
        "--file_for_eval",
        type=str,
        help="The input excel file path to evaluate, for example : 'test.xlxs', which should provide in the same folder as eval_runner.py",
    )

    parser.add_argument(
        "--test-category",
        nargs="+",
        type=str,
        help="A list of test categories to run the evaluation on, currently support simple, multiple_function, all. If all is provided, both simple and multiple test categories will be run.",
    )

    args = parser.parse_args()
    current_directory = os.path.dirname(os.path.abspath(__file__))
    print(f"The current directory is {current_directory}\n")

    print(f"Start converting the original file to BFCL format...\n")

    # original input file path for evaluation
    original_yewu_file = os.path.join(current_directory, "original_yewu_file.jsonl")

    # convert the original excel file to jsonl format
    clean_and_convert_excel_to_jsonl(args.file_for_eval, original_yewu_file)

    # Construct the paths for the output files-- simple and multiple
    converted_simple_func_file = os.path.join(
        current_directory, "converted_simple_func_file.jsonl"
    )
    converted_multiple_func_file = os.path.join(
        current_directory, "converted_multiple_func_file.jsonl"
    )
    converted_simple_model_result_file = os.path.join(
        current_directory, "converted_simple_model_result_file.jsonl"
    )
    converted_multiple_model_result_file = os.path.join(
        current_directory, "converted_multiple_model_result_file.jsonl"
    )
    converted_simple_possible_answer = os.path.join(
        current_directory, "converted_simple_possible_answer.jsonl"
    )
    converted_multiple_possible_answer = os.path.join(
        current_directory, "converted_multiple_possible_answer.jsonl"
    )

    convert_to_BFCL_format_input(
        original_yewu_file,
        converted_simple_func_file,
        converted_multiple_func_file,
        converted_simple_model_result_file,
        converted_multiple_model_result_file,
        converted_simple_possible_answer,
        converted_multiple_possible_answer,
    )
    print(
        f"â­ï¸Converted simple AST func_description saved as: {converted_simple_func_file}\n"
    )
    print(
        f"â­ï¸Converted multiple AST func_description saved as: {converted_multiple_func_file}\n"
    )
    print(
        f"â­ï¸Converted simple model result saved as: {converted_simple_model_result_file}\n"
    )
    print(
        f"â­ï¸Converted multiple model result saved as: {converted_multiple_model_result_file}\n"
    )
    print(
        f"â­ï¸Converted simple possible answer saved as: {converted_simple_possible_answer}\n"
    )

    print(
        f"â­ï¸Converted multiple possible answer saved as: {converted_multiple_possible_answer}\n"
    )
    print("####----------------------###")
    # print(type(converted_multiple_ast_test_file))
    model_names = "ernie-series-assitant-api-FC"  # hardcoded!!!!â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸

    provided_test_categories = args.test_category
    if not args.test_category:
        raise ValueError(
            "Please provide at least one test category to run the evaluation on."
        )

    supported_categories = ["simple", "multiple_function", "all"]
    test_categories = []
    for test_category in args.test_category:
        if test_category not in supported_categories:
            raise ValueError(f"Test category {test_category} is not supported.")
        if test_category == "all":
            # avoid duplicate logic processing:
            for test_cat in ["simple", "multiple_function"]:
                if test_cat not in test_categories:
                    test_categories.append(
                        test_cat
                    )  # ä¿®æ”¹äº†è¿™é‡Œï¼Œåº”è¯¥æ˜¯ test_cat è€Œä¸æ˜¯ test_category
            break
        else:
            if test_category not in test_categories:
                test_categories.append(test_category)

    runner(
        model_names,
        test_categories,
        converted_simple_func_file,  # å¯èƒ½æ˜¯ç©ºçš„å“¦
        converted_multiple_func_file,
        converted_simple_model_result_file,  # å¯èƒ½æ˜¯ç©ºçš„å“¦
        converted_multiple_model_result_file,
        converted_simple_possible_answer,  # å¯èƒ½æ˜¯ç©ºçš„å“¦
        converted_multiple_possible_answer,
    )

    # for target_test_category in test_categories: # either simple or multiple_function
    #     if target_test_category == "simple":
    #         runner(model_names, [target_test_category], converted_simple_ast_test_file)
    #         print(f"ğŸ” Running evaluation on {target_test_category} test category\n")
    #     elif target_test_category == "multiple_function":
    #         runner(model_names,[target_test_category], converted_multiple_ast_test_file)
    #         print(f"ğŸ” Running evaluation on {target_test_category} test category\n")
